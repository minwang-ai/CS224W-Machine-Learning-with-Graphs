{"cells":[{"cell_type":"markdown","metadata":{"id":"XuXWJLEm2UWS"},"source":["# **XCS224W - Colab 0**\n","\n","Colab 0 **will not be graded**, so you don't need to hand in this notebook. That said, we highly recommend that you work through this notebook, so you can get familiar with the basic concepts of graph mining and Graph Neural Networks.\n","\n","In this Colab, we will introduce two packages, [NetworkX](https://networkx.org/documentation/stable/) and [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/).\n","\n","For the PyTorch Geometric section, you don't need to understand all of the details yet. Concepts and implementations of graph neural network will be covered in future lectures and Colabs."]},{"cell_type":"markdown","metadata":{"id":"8gzsP50bF6Gb"},"source":["\n","# NetworkX Tutorial\n","\n","NetworkX is one of the most frequently used Python packages to create, manipulate, and mine graphs.\n","\n","The main parts of this tutorial are adapted from https://colab.research.google.com/github/jdwittenauer/ipython-notebooks/blob/master/notebooks/libraries/NetworkX.ipynb#scrollTo=zA1OO6huHeV6"]},{"cell_type":"markdown","metadata":{"id":"Nwwq0nSdmsOL"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QeqN7MHvH1OA"},"outputs":[],"source":["# Import the NetworkX package\n","import networkx as nx"]},{"cell_type":"markdown","metadata":{"id":"MCIeGAfLfAMK"},"source":["## Graph\n","NetworkX provides several classes to store different types of graphs, such as directed and undirected graphs. It also provides classes to create multigraphs (both directed and undirected).\n","\n","For more information, please refer to [NetworkX graph types](https://networkx.org/documentation/stable/reference/classes/index.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSI_n6P-e0PW"},"outputs":[],"source":["# Create an undirected graph G\n","G = nx.Graph()\n","print(G.is_directed())\n","\n","# Create a directed graph H\n","H = nx.DiGraph()\n","print(H.is_directed())\n","\n","# Add graph level attribute\n","G.graph[\"Name\"] = \"Bar\"\n","print(G.graph)"]},{"cell_type":"markdown","metadata":{"id":"x0pLs0-Ka9j8"},"source":["## Node\n","\n","Nodes (with attributes) can be easily added to NetworkX graphs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQ8ApAL5H1GB"},"outputs":[],"source":["# Add one node with node level attributes\n","G.add_node(0, feature=0, label=0)\n","\n","# Get attributes of the node 0\n","node_0_attr = G.nodes[0]\n","print(\"Node 0 has the attributes {}\".format(node_0_attr))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btOdMYqJaKia"},"outputs":[],"source":["# Add multiple nodes with attributes\n","G.add_nodes_from([\n","  (1, {\"feature\": 1, \"label\": 1}),\n","  (2, {\"feature\": 2, \"label\": 2})\n","])\n","\n","# Loop through all the nodes\n","# Set data=True will return node attributes\n","for node in G.nodes(data=True):\n","  print(node)\n","\n","# Get number of nodes\n","num_nodes = G.number_of_nodes()\n","print(\"G has {} nodes\".format(num_nodes))"]},{"cell_type":"markdown","metadata":{"id":"0AdoaZPgbRis"},"source":["## Edge\n","\n","Similar to nodes, edges (with attributes) can be easily added to NetworkX graphs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0szH5F6EH079"},"outputs":[],"source":["# Add one edge with edge weight 0.5\n","G.add_edge(0, 1, weight=0.5)\n","\n","# Get attributes of the edge (0, 1)\n","edge_0_1_attr = G.edges[(0, 1)]\n","print(\"Edge (0, 1) has the attributes {}\".format(edge_0_1_attr))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QRTmi4EUaf_I"},"outputs":[],"source":["# Add multiple edges with edge weights\n","G.add_edges_from([\n","  (1, 2, {\"weight\": 0.3}),\n","  (2, 0, {\"weight\": 0.1})\n","])\n","\n","# Loop through all the edges\n","# Here there is no data=True, so only the edge will be returned\n","for edge in G.edges():\n","  print(edge)\n","\n","# Get number of edges\n","num_edges = G.number_of_edges()\n","print(\"G has {} edges\".format(num_edges))"]},{"cell_type":"markdown","metadata":{"id":"9u1Utjn4bc7k"},"source":["## Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lykPzFOEahuP"},"outputs":[],"source":["# Draw the graph\n","nx.draw(G, with_labels = True)"]},{"cell_type":"markdown","metadata":{"id":"_Q6YTP2FDbOS"},"source":["## Node Degree and Neighbors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFA3B6Z_DE3q"},"outputs":[],"source":["node_id = 1\n","\n","# Degree of node 1\n","print(\"Node {} has degree {}\".format(node_id, G.degree[node_id]))\n","\n","# Get neighbor of node 1\n","for neighbor in G.neighbors(node_id):\n","  print(\"Node {} has neighbor {}\".format(node_id, neighbor))"]},{"cell_type":"markdown","metadata":{"id":"4gVRVckZeSdA"},"source":["## Other Functionalities\n","\n","NetworkX also provides plenty of useful methods to study graphs.\n","\n","Here is an example of getting the [PageRank](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html#networkx.algorithms.link_analysis.pagerank_alg.pagerank) of nodes (we will introduce PageRank in Module 1: Traditional Methods for ML on Graphs)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gZfQ82Wiuvv"},"outputs":[],"source":["num_nodes = 4\n","# Create a new path like graph and change it to a directed graph\n","G = nx.DiGraph(nx.path_graph(num_nodes))\n","nx.draw(G, with_labels = True)\n","\n","# Get the PageRank\n","pr = nx.pagerank(G, alpha=0.8)\n","pr"]},{"cell_type":"markdown","metadata":{"id":"RrIFCJnlvGkg"},"source":["## Documentation"]},{"cell_type":"markdown","metadata":{"id":"_7PBwhIKu3et"},"source":["You can explore more NetworkX functions through its [documentation](https://networkx.org/documentation/stable/)."]},{"cell_type":"markdown","metadata":{"id":"1yZmVdW04776"},"source":["# PyTorch Geometric Tutorial\n","## Introduction\n","\n","Recently, deep learning on graphs has emerged as one of the hottest research fields in the deep learning community.\n","**Graph Neural Networks (GNNs)** aim to generalize classical deep learning concepts to structured relational data (distinct from images or texts), enabling neural networks to reason about objects and their relations. This tutorial will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks. Don't worry if you don't yet understand some GNN specific concepts such as `GCNConv` -- we will cover this and more, starting with Module 2: Introduction to Graph Neural Networks :). \n","\n","We begin by introducing the powerfurl **[PyTorch Geometric (PyG) library](https://github.com/rusty1s/pytorch_geometric)**.\n","PyTorch Geometric is an extension to the popular deep learning framework [PyTorch](https://pytorch.org/) and consists of various methods and utilities to ease the implementation of Graph Neural Networks. \n","\n","Following [Kipf et al. (2017)](https://arxiv.org/abs/1609.02907), we dive into the world of GNNs by looking at a simple graph-structured example, the well-known [**Zachary's karate club network**](https://en.wikipedia.org/wiki/Zachary%27s_karate_club). This graph describes a social network of 34 members of a karate club where a link exists between members if they have interacted outside the club. For this exploration, we are interested in detecting communities that arise from the member's interaction.\n","\n","**Note**: This tutorial is adapted from https://colab.research.google.com/drive/1h3-vJGRVloF5zStxL5I0rSy4ZUPNsjy8?usp=sharing#scrollTo=ci-LpZWhRJoI by [Matthias Fey](https://rusty1s.github.io/#/)"]},{"cell_type":"markdown","metadata":{"id":"Oi2JtnX18ezY"},"source":["### Setup\n","The installation of PyG on Colab can be a little bit tricky. First let us check which version of PyTorch you are running"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WU7NCZtQ6msa"},"outputs":[],"source":["import torch\n","print(\"PyTorch has version {}\".format(torch.__version__))"]},{"cell_type":"markdown","metadata":{"id":"n7gMjMCT677n"},"source":["Now, we will download the necessary packages for PyG. Before executing the cell below, make sure that your version of torch matches the output from the cell above. Namely, if the above cell prints \"PyTorch has version 1.11.0+cu113\", the urls for the installation of **torch-scatter** and **torch-sparse** should include **torch-1.11.0+cu113** -- in case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tRNEKu-R66Cw"},"outputs":[],"source":["# Install torch geometric\n","!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n","!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n","!pip install torch-geometric"]},{"cell_type":"markdown","metadata":{"id":"EfiMQ4itvhQq"},"source":["Run the following cell to fix a strange issue with Colab and the python-louvain package"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52vBk7FTvc4E"},"outputs":[],"source":["!pip uninstall python-louvain --y\n","!pip install python-louvain"]},{"cell_type":"markdown","metadata":{"id":"HLxnaKsN8GVf"},"source":["## Visualization Helper Script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qywlcjyr8USw"},"outputs":[],"source":["# Helper function for visualization.\n","%matplotlib inline\n","import torch\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","\n","# Visualization function for NX graph or PyTorch tensor\n","def visualize(h, color, epoch=None, loss=None):\n","    plt.figure(figsize=(7,7))\n","    plt.xticks([])\n","    plt.yticks([])\n","\n","    if torch.is_tensor(h):\n","        h = h.detach().cpu().numpy()\n","        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n","        if epoch is not None and loss is not None:\n","            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n","    else:\n","        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n","                         node_color=color, cmap=\"Set2\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"g3uPffzbyqn9"},"source":["## Dataset\n","\n","PyTorch Geometric provides easy access to the [**Zachary's karate club network**](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) via the [`torch_geometric.datasets`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets) subpackage:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YrpL9CtS7nx2"},"outputs":[],"source":["from torch_geometric.datasets import KarateClub\n","\n","dataset = KarateClub()\n","print(f'Dataset: {dataset}:')\n","print('======================')\n","print(f'Number of graphs: {len(dataset)}')\n","print(f'Number of features: {dataset.num_features}')\n","print(f'Number of classes: {dataset.num_classes}')"]},{"cell_type":"markdown","metadata":{"id":"lCeRGa2q7sdl"},"source":["After initializing the [`KarateClub`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.KarateClub) dataset, we first inspect some of its properties.\n","For example, we see that this dataset holds exactly **one graph** and each node in this dataset is assigned a **34-dimensional feature vector** (which uniquely describes the members of the karate club).\n","Furthermore, the graph holds exactly **4 classes** that represent the community each node belongs to.\n","\n","Let's now look at the underlying graph in more detail:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTLapYhP7uCn"},"outputs":[],"source":["data = dataset[0]  # Get the first graph object.\n","\n","print(data)\n","print('==============================================================')\n","\n","# Gather some statistics about the graph.\n","print(f'Number of nodes: {data.num_nodes}')\n","print(f'Number of edges: {data.num_edges}')\n","print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n","print(f'Number of training nodes: {data.train_mask.sum()}')\n","print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n","print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n","print(f'Contains self-loops: {data.contains_self_loops()}')\n","print(f'Is undirected: {data.is_undirected()}')"]},{"cell_type":"markdown","metadata":{"id":"lIzbIoc-y8J4"},"source":["## Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"I5zhmKIH72Rf"},"source":["Each graph in PyTorch Geometric is represented by a single [`Data`](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data) object, which holds all the information to describe the graph representation.\n","We can print the data object anytime via `print(data)` to receive a short summary about its attributes and their shapes:\n","```\n","Data(edge_index=[2, 156], x=[34, 34], y=[34], train_mask=[34])\n","```\n","This graph `data` object holds 4 attributes:\n","(1) The `edge_index` property holds the information about the **graph's connectivity or edges**, where each edge is represented as a tuple of source and destination node indices. For undirected graphs, each edge is represented twice to capture both edge directions. Hence, our graph has 78 edges; PyG refers to (2) **node features** as the variable `x` (each of the 34 nodes is assigned a 34-dim feature vector) and (3) **node labels** as `y` (each node is assigned to exactly one class); (4) finally, the `train_mask` attribute describes the nodes for which we already know their community assigments.\n","In total, we are only aware of the ground-truth labels of 4 nodes (one for each community), and our task is to infer the community assignment for the remaining nodes.\n","\n","The `data` object also provides some **utility functions** to infer some basic properties of the underlying graph.\n","For example, we can easily infer whether there exists isolated nodes in the graph (*i.e.* there exists no edge to any node), whether the graph contains self-loops (*i.e.*, $(v, v) \\in \\mathcal{E}$), or whether the graph is undirected (*i.e.*, for each edge $(v, w) \\in \\mathcal{E}$ there also exists the edge $(w, v) \\in \\mathcal{E}$)."]},{"cell_type":"markdown","metadata":{"id":"nLLsT0ROzffp"},"source":["## Edge Index"]},{"cell_type":"markdown","metadata":{"id":"wQJyi9OB8dh_"},"source":["By printing `edge_index`, we can further understand how PyG represents graph connectivity internally."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICeUCweMeq9m"},"outputs":[],"source":["from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","edge_index = data.edge_index\n","print(edge_index.t())"]},{"cell_type":"markdown","metadata":{"id":"PG86_w6We_Hx"},"source":["As described above in **Data Exploration**, for each edge, `edge_index` holds a tuple of two node indices, where the first value describes the node index of the source node and the second value describes the node index of the destination node.\n","\n","This representation is known as the **COO format (coordinate format)** commonly used for representing sparse matrices.\n","Instead of holding the adjacency information in a dense representation $\\mathbf{A} \\in \\{ 0, 1 \\}^{|\\mathcal{V}| \\times |\\mathcal{V}|}$, PyG represents graphs sparsely, which refers to only holding the coordinates/values for which entries in $\\mathbf{A}$ are non-zero.\n","\n","We can further visualize the graph by converting it to the `networkx` library format. `Networkx` serves not only as a rich tool for graph manipulation, but also as a powerful tool for visualization:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KfJHtlV8h3W"},"outputs":[],"source":["from torch_geometric.utils import to_networkx\n","\n","G = to_networkx(data, to_undirected=True)\n","visualize(G, color=data.y)"]},{"cell_type":"markdown","metadata":{"id":"NUdHZY2u8vn3"},"source":["## Implementing Graph Neural Networks\n","\n","After learning about PyG's data handling, it's time to implement our first Graph Neural Network!\n","\n","For this, we will use one of the most simple GNN operators, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)).\n","\n","PyG implements this layer via [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv), which can be executed by passing in the node feature representation `x` and the COO graph connectivity representation `edge_index`.\n","\n","With this, we are ready to create our first Graph Neural Network by defining our network architecture in a `torch.nn.Module` class:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tQGQF8r8zIr"},"outputs":[],"source":["import torch\n","from torch.nn import Linear\n","from torch_geometric.nn import GCNConv\n","\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self):\n","        super(GCN, self).__init__()\n","        torch.manual_seed(12345)\n","        self.conv1 = GCNConv(dataset.num_features, 4)\n","        self.conv2 = GCNConv(4, 4)\n","        self.conv3 = GCNConv(4, 2)\n","        self.classifier = Linear(2, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        h = self.conv1(x, edge_index)\n","        h = h.tanh()\n","        h = self.conv2(h, edge_index)\n","        h = h.tanh()\n","        h = self.conv3(h, edge_index)\n","        h = h.tanh()  # Final GNN embedding space.\n","        \n","        # Apply a final (linear) classifier.\n","        out = self.classifier(h)\n","\n","        return out, h\n","\n","model = GCN()\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"5zgbaD5P8_M_"},"source":["We first initialize all of the building blocks in `__init__`, and then define the computation flow of our network in `forward`.\n","Our GNN is defined by stacking **three graph convolution layers**, which corresponds to aggregating 3-hop neighborhood information around each node (all nodes up to 3 \"hops\" away).\n","In addition, the `GCNConv` layers reduce the node feature dimensionality to $2$, *i.e.*, $34 \\rightarrow 4 \\rightarrow 4 \\rightarrow 2$. Each `GCNConv` layer is enhanced by a [tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html?highlight=tanh#torch.nn.Tanh) non-linearity.\n","\n","After that, we apply a single linear transformation ([`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear)) that acts as a classifier to map our nodes to 1 out of the 4 classes/communities.\n","\n","We return both the output of the final classifier as well as the final node embeddings produced by our GNN.\n","We proceed to initialize our final model via `GCN()`, and printing our model produces a summary of all its used sub-modules."]},{"cell_type":"markdown","metadata":{"id":"lVGfsrhehucb"},"source":["## Pre-Trained Embeddings\n","\n","Before training the weights of our model, we can already visualize the embeddings generated by GCN."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48uhs_0j9AMX"},"outputs":[],"source":["model = GCN()\n","\n","_, h = model(data.x, data.edge_index)\n","print(f'Embedding shape: {list(h.shape)}')\n","\n","visualize(h, color=data.y)"]},{"cell_type":"markdown","metadata":{"id":"nDtJ9Zjw9I_Y"},"source":["Remarkably, even before training, the model produces an embedding of nodes that closely resembles the community-structure of the graph.\n","Nodes of the same color (community) are already closely clustered together in the embedding space, although the weights of our model are initialized **completely at random** and we have not yet performed any training!\n","This leads to the conclusion that GNNs introduce a strong inductive bias based on the graph structure itself, which naturally leads to similar embeddings for near by nodes in the graph.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oqzA4l9_ioiz"},"source":["## Training on the Karate Club Network\n","\n","Can we do better? Let's look at an example on how to train our network parameters based on the community assignments of the 4 known nodes in the graph (one for each community):\n","\n","Since everything in our model is differentiable and parameterized, we can add node labels, train the model, and observe how the embeddings react.\n","Here, we make use of a semi-supervised learning procedure: We train with the supervision of one node per class, but are allowed to make use of the complete input graph data for the generation of node embeddings. Our goal is to then predict the labels of the unknown nodes.\n","\n","Training our model is very similar to any other PyTorch model.\n","In addition to defining our network architecture, we define a loss critertion ([`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)) and initialize a stochastic gradient optimizer ([`Adam`](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam)).\n","After that, we perform multiple rounds of optimization, where each round consists of a forward and backward pass to compute the gradients of our model parameters w.r.t. to the loss derived from the forward pass.\n","If you are familiar with PyTorch, this scheme should appear very familar to you. \n","Otherwise, the PyTorch docs provide [a good introduction on how to train a neural network in PyTorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-loss-function-and-optimizer).\n","\n","Note that our semi-supervised learning scenario is achieved by the following line:\n","```\n","loss = criterion(out[data.train_mask], data.y[data.train_mask])\n","```\n","While we compute node embeddings for all of our nodes, we **only make use of the training node embeddings for computing the loss**.\n","This is implemented by filtering the output of the classifier `out` and ground-truth labels `data.y` to only contain the nodes in the `train_mask`.\n","\n","Let us now start training and see how our node embeddings evolve over time (best experienced by explicitely running the code):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FI3DETGi9ND6"},"outputs":[],"source":["import time\n","from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 430})'''))\n","\n","model = GCN()\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n","\n","def train(data):\n","    optimizer.zero_grad()  # Clear gradients.\n","    out, h = model(data.x, data.edge_index)  # Perform a single forward pass.\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","    loss.backward()  # Derive gradients.\n","    optimizer.step()  # Update parameters based on gradients.\n","    return loss, h\n","\n","for epoch in range(401):\n","    loss, h = train(data)\n","    # Visualize the node embeddings every 10 epochs\n","    if epoch % 10 == 0:\n","        visualize(h, color=data.y, epoch=epoch, loss=loss)\n","        time.sleep(0.3)"]},{"cell_type":"markdown","metadata":{"id":"F2B3X6tf9YpS"},"source":["As one can see, our 3-layer GCN model manages to linearly separating the communities and classifies most of the nodes correctly.Furthermore, we did this all with a few lines of code, thanks to the PyTorch Geometric library! \n","\n","Congratulations on completing your first Colab for XCS224W. Hopefully you now have a flavor of the assignments to come and an exposure to the rich functionalities of the libraries that we will use throughout the course!  \n"]},{"cell_type":"markdown","metadata":{"id":"E9bELRjibIRO"},"source":["## Documentation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OmqyWVNObNcK"},"source":["You can explore more PyG functions through its [documentation](https://pytorch-geometric.readthedocs.io/en/latest/)."]},{"cell_type":"markdown","metadata":{"id":"T10lC1_Eivqp"},"source":["## Building + Debugging Notes\n","\n","Now that we have seen the basics of the PyG, and the tutorial was relative simple, fully reviewed and tested, let's see what we can do if things don't go as we expect.\n","\n","Firstly, while working through future Colabs, to make sure you have enough GPU credits left for all assignments, we strongly encourage making sure that your notebook is using the CPU runtime during development and debugging. You can change the notebook runtime by selecting `Runtime` and then `Change runtime type`. From the dropdown, select `None` as the `hardware accelerator`. Once you have a full initial version and want to train the models, it is helpful to start by only running one epoch or even just a couple of batch iterations. This way you can check that the code fully executes and all your tensor shapes and logic match up, while also tracking expected behavior, such as a decreasing training loss. Remember to comment out / save the default number of epochs that we provide you.\n","\n","While the hope is that everything goes right the first time around, usually things don't go quite as planned and we see ourselves burried in all kind of errors. To finish this tutorial, we explore how we could investigate and fix some of the errors that you may run into.\n","\n","In the next cell, we have created a version of the above GCN model, with some straightforward and subtle issues in it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yM1Wxs-pmluE"},"outputs":[],"source":["import torch\n","from torch.nn import Linear, Sequential, Tanh\n","from torch_geometric.nn import GCNConv\n","\n","\n","class BuggyGCN(torch.nn.Module):\n","    def __init__(self, args, dataset):\n","        super(BuggyGCN, self).__init__()\n","        torch.manual_seed(12345)\n","\n","        self.conv = Sequential(\n","            GCNConv(dataset.num_features, args.hidden_size),\n","            Tanh(),\n","            GCNConv(args.hidden_size, args.hidden_size),\n","            Tanh(),\n","            GCNConv(args.hidden_size, args.hidden_size)\n","        )\n","\n","        self.classifier = Linear(2, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        h = self.conv(x, edge_index) # GNN embedding space.\n","        \n","        # Apply a final (linear) classifier.\n","        out = self.classifier(h)\n","\n","        return out, h"]},{"cell_type":"markdown","metadata":{"id":"DSenu33UxyCA"},"source":["Now, let's create the training arguments, pass them to the training code, and see what happens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWj20TMAynSo"},"outputs":[],"source":["# Feel free to change the following parameters\n","args = {\n","    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    'hidden_size': 4,\n","    'epochs': 401,\n","    'lr': 0.01,\n","    'sleep': 0.2\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GVYy6pZj2D3"},"outputs":[],"source":["from tqdm import tqdm\n","import time\n","\n","model = BuggyGCN(args, dataset)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])  # Define optimizer.\n","\n","def train(data):\n","    optimizer.zero_grad()  # Clear gradients.\n","    out, h = model(data.x, data.edge_index)  # Perform a single forward pass.\n","    loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","    loss.backward()  # Derive gradients.\n","    optimizer.step()  # Update parameters based on gradients.\n","    return loss, h\n","\n","with tqdm(range(args[\"epochs\"]), unit=\"batch\") as tepoch:\n","  for epoch in tepoch:  \n","      loss, h = train(data)\n","      tepoch.set_description(f'Epoch {epoch} - Loss {loss.item()}')\n","      time.sleep(args['sleep'])\n"]},{"cell_type":"markdown","metadata":{"id":"u5kQZFYyZU33"},"source":["You now should get the following error:\n","\n","```\n","<ipython-input-76-22a7cac65169> in __init__(self, args, dataset)\n","     10 \n","     11         self.conv = Sequential(\n","---> 12             GCNConv(dataset.num_features, args.hidden_size),\n","     13             Tanh(),\n","     14             GCNConv(args.hidden_size, args.hidden_size),\n","\n","AttributeError: 'dict' object has no attribute 'hidden_size'\n","```\n","As can be seen in the call stack, the issue seems to be with the `args` dictionary, and at first sight it does not seem to contain the `hidden_size` key. One way to check this would be to print the `args` variable before the error happens. Let's do that in the next cell. Execute the new model below and then again the training cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NjvOKFC2oOA"},"outputs":[],"source":["import torch\n","from torch.nn import Linear, Sequential, Tanh\n","from torch_geometric.nn import GCNConv\n","\n","\n","class BuggyGCN(torch.nn.Module):\n","    def __init__(self, args, dataset):\n","        super(BuggyGCN, self).__init__()\n","        torch.manual_seed(12345)\n","\n","        print(f'args: {args}')\n","        \n","        self.conv = Sequential(\n","            GCNConv(dataset.num_features, args.hidden_size),\n","            Tanh(),\n","            GCNConv(args.hidden_size, args.hidden_size),\n","            Tanh(),\n","            GCNConv(args.hidden_size, args.hidden_size)\n","        )\n","\n","        self.classifier = Linear(2, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        h = self.conv(x, edge_index) # GNN embedding space.\n","        \n","        # Apply a final (linear) classifier.\n","        out = self.classifier(h)\n","\n","        return out, h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxC4lCICmmnd"},"outputs":[],"source":["model = BuggyGCN(args, dataset)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])  # Define optimizer.\n","\n","with tqdm(range(args[\"epochs\"]), unit=\"batch\") as tepoch:\n","  for epoch in tepoch:  \n","      loss, h = train(data)\n","      tepoch.set_description(f'Epoch {epoch} - Loss {loss.item()}')\n","      time.sleep(args['sleep'])"]},{"cell_type":"markdown","metadata":{"id":"WXEYwKVJackb"},"source":["As can be seen in the output the `args` variable looks like this:\n","\n","```\n","args: {'device': device(type='cpu'), 'hidden_size': 4, 'epochs': 401, 'lr': 0.01, 'sleep': 0.2}\n","```\n","\n","so obviously it has the `hidden_size` key. How come then it's not working, considering that the `dataset.num_features` seems to work? Well, python is an untyped programming language and objects are not always the same type you expect. As `dataset` is an object of type ` torch_geometric.data.Dataset` containing the property `num_features`, `args` is a dictionary and unlike other programming languages, such as Javascript, accessing the keys with `.` doesn't work, unless you wrap it in a class end expose the keys as properties dynamically. \n","\n","Let's correct this issue and use the python way of accessing keys, i.e. `dict[\"key\"]`. After updating the model we try  running the training loop again to see what happens next."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXKb7JOb3Uiv"},"outputs":[],"source":["import torch\n","from torch.nn import Linear, Sequential, Tanh\n","from torch_geometric.nn import GCNConv\n","\n","\n","class BuggyGCN(torch.nn.Module):\n","    def __init__(self, args, dataset):\n","        super(BuggyGCN, self).__init__()\n","        torch.manual_seed(12345)\n","        \n","        self.conv = Sequential(\n","            GCNConv(dataset.num_features, args[\"hidden_size\"]),\n","            Tanh(),\n","            GCNConv(args[\"hidden_size\"], args[\"hidden_size\"]),\n","            Tanh(),\n","            GCNConv(args[\"hidden_size\"], args[\"hidden_size\"])\n","        )\n","\n","        self.classifier = Linear(2, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        h = self.conv(x, edge_index) # GNN embedding space.\n","        \n","        # Apply a final (linear) classifier.\n","        out = self.classifier(h)\n","\n","        return out, h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twY6IdF-mmnd"},"outputs":[],"source":["model = BuggyGCN(args, dataset)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])  # Define optimizer.\n","\n","with tqdm(range(args[\"epochs\"]), unit=\"batch\") as tepoch:\n","  for epoch in tepoch:  \n","      loss, h = train(data)\n","      tepoch.set_description(f'Epoch {epoch} - Loss {loss.item()}')\n","      time.sleep(args['sleep'])"]},{"cell_type":"markdown","metadata":{"id":"yG_pNP8xcSq6"},"source":["Oh no, another error:\n","\n","```\n","<ipython-input-83-8d1339d52472> in forward(self, x, edge_index)\n","     20 \n","     21     def forward(self, x, edge_index):\n","---> 22         h = self.conv(x, edge_index) # GNN embedding space.\n","     23 \n","     24         # Apply a final (linear) classifier.\n","\n","TypeError: forward() takes 2 positional arguments but 3 were given\n","```\n","\n","As can be seen in the call stack, the `self.conv.forward` method seems to expect just one parameter (the error takes the `self` reference also into account). So let's explore how we could get the definition of the `self.conv.forward` method and see what is happening. \n","\n","For this we introduce [The Python Debugger](https://docs.python.org/3/library/pdb.html), a powerful tool for debugging python programs. To do so, let's add the line `pdb.set_trace()` before the line causing the problem to stop the execution and be able to analyse the issue. The pdb window will open below the executing cell and we could execute some python code without breaking the execution thread. In order to get a function signature please execute the following lines one after each other in the pdb window, ending with the exit command:\n","\n","```\n","import inspect\n","print(inspect.signature(self.conv.forward))\n","exit\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycAWcVf86_uW"},"outputs":[],"source":["import torch\n","import pdb\n","from torch.nn import Linear, Sequential, Tanh\n","from torch_geometric.nn import GCNConv\n","\n","\n","class BuggyGCN(torch.nn.Module):\n","    def __init__(self, args, dataset):\n","        super(BuggyGCN, self).__init__()\n","        torch.manual_seed(12345)\n","        \n","        self.conv = Sequential(\n","            GCNConv(dataset.num_features, args[\"hidden_size\"]),\n","            Tanh(),\n","            GCNConv(args[\"hidden_size\"], args[\"hidden_size\"]),\n","            Tanh(),\n","            GCNConv(args[\"hidden_size\"], args[\"hidden_size\"])\n","        )\n","\n","        self.classifier = Linear(2, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","\n","        pdb.set_trace()\n","        \n","        h = self.conv(x, edge_index) # GNN embedding space.\n","        \n","        # Apply a final (linear) classifier.\n","        out = self.classifier(h)\n","\n","        return out, h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WVyf54Smmne"},"outputs":[],"source":["model = BuggyGCN(args, dataset)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])  # Define optimizer.\n","\n","with tqdm(range(args[\"epochs\"]), unit=\"batch\") as tepoch:\n","  for epoch in tepoch:  \n","      loss, h = train(data)\n","      tepoch.set_description(f'Epoch {epoch} - Loss {loss.item()}')\n","      time.sleep(args['sleep'])"]},{"cell_type":"markdown","metadata":{"id":"WwKOBiNIe9bV"},"source":["The pdb output should look like this:\n","\n","```\n","-> h = self.conv(x, edge_index) # GNN embedding space.\n","(Pdb) import inspect\n","(Pdb) print(inspect.signature(self.conv.forward))\n","(input)\n","(Pdb) exit\n","```\n","\n","We see that the `Sequence.forward` method accepts just one parameter, `input`, exposing a limitation of the `Sequence` class when being used with modules that expect more than one parameter in their `forward` method. Since the `GCNConv` is a PyG module, we cannot change it, so let's get rid of the `Sequence` and use `ModuleList` instead. Execute the next cells, containaing the updated definition of the model. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIuPB4c8GLUc"},"outputs":[],"source":["import torch\n","import pdb\n","from torch.nn import Linear, Sequential, Tanh, ModuleList\n","from torch_geometric.nn import GCNConv\n","\n","\n","class BuggyGCN(torch.nn.Module):\n","    def __init__(self, args, dataset):\n","        super(BuggyGCN, self).__init__()\n","        torch.manual_seed(12345)\n","        \n","        self.convs = ModuleList([\n","            GCNConv(dataset.num_features, args[\"hidden_size\"]),\n","            GCNConv(args[\"hidden_size\"], args[\"hidden_size\"]),\n","            GCNConv(args[\"hidden_size\"], args[\"hidden_size\"])\n","        ])\n","\n","        self.classifier = Linear(2, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","\n","        h = x\n","        for i in range(len(self.convs)):\n","          h = self.convs[i](h, edge_index)\n","          h = h.tanh()\n","\n","        # h = GNN embedding space.\n","        \n","        # Apply a final (linear) classifier.\n","        out = self.classifier(h)\n","\n","        return out, h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AqneVKHmmnf"},"outputs":[],"source":["model = BuggyGCN(args, dataset)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])  # Define optimizer.\n","\n","with tqdm(range(args[\"epochs\"]), unit=\"batch\") as tepoch:\n","  for epoch in tepoch:  \n","      loss, h = train(data)\n","      tepoch.set_description(f'Epoch {epoch} - Loss {loss.item()}')\n","      time.sleep(args['sleep'])"]},{"cell_type":"markdown","metadata":{"id":"OIurfxO-gNyN"},"source":["Not again, another error. When does it stop? Fortunately, this is the last error we built-in. So the error now looks like this:\n","\n","```\n","<ipython-input-92-7a05cd42a108> in forward(self, x, edge_index)\n","     28 \n","     29         # Apply a final (linear) classifier.\n","---> 30         out = self.classifier(h)\n","     31 \n","     32         return out, h\n","\n","RuntimeError: mat1 and mat2 shapes cannot be multiplied (34x4 and 2x4)\n","```\n","\n","This is a matrix multiplication shape error, an error you will likely run into many time. Let's see how we can investigate this in Google Colab. We know that we could use `print` but that makes our code a bit messy and we also have seen `pdb` in action, so let's use it again to break the execution before the line presented in the call stack. With the exection stopped, we can inspect global and local variables two different ways. Firstly, we can use the Google Colab Variables window, which can be found on the left side bar as $\\{x\\}$, above the folder icon. Using `pdb` we can also print variable information using `print(var.info)`.\n","\n","Execute the following cell containing the modifed model and then the training loop. After executution stops, first use the Variable window to inspect the dimensions of the matrix `h`. Then, using pdb, confirm `h`'s dimension and print the definition of the `self.classifier` layer:\n","\n","```\n","p h.shape\n","p self.classifier\n","exit\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8YyyUrJH8hx"},"outputs":[],"source":["import torch\n","import pdb\n","from torch.nn import Linear, Sequential, Tanh, ModuleList\n","from torch_geometric.nn import GCNConv\n","\n","\n","class BuggyGCN(torch.nn.Module):\n","    def __init__(self, args, dataset):\n","        super(BuggyGCN, self).__init__()\n","        torch.manual_seed(12345)\n","        \n","        self.convs = ModuleList([\n","            GCNConv(dataset.num_features, args[\"hidden_size\"]),\n","            GCNConv(args[\"hidden_size\"], args[\"hidden_size\"]),\n","            GCNConv(args[\"hidden_size\"], args[\"hidden_size\"])\n","        ])\n","\n","        self.classifier = Linear(2, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","\n","        h = x\n","        for i in range(len(self.convs)):\n","          h = self.convs[i](h, edge_index)\n","          h = h.tanh()\n","\n","        # h = GNN embedding space.\n","        \n","        pdb.set_trace()\n","        # Apply a final (linear) classifier.\n","        out = self.classifier(h)\n","\n","        return out, h"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"qKDaAXL3mmng"},"outputs":[],"source":["model = BuggyGCN(args, dataset)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])  # Define optimizer.\n","\n","with tqdm(range(args[\"epochs\"]), unit=\"batch\") as tepoch:\n","  for epoch in tepoch:  \n","      loss, h = train(data)\n","      tepoch.set_description(f'Epoch {epoch} - Loss {loss.item()}')\n","      time.sleep(args['sleep'])"]},{"cell_type":"markdown","metadata":{"id":"thv6UWaOiRwn"},"source":["The dimension of the `h` variable should be, when using the default values given in the `args` dictionary, `(34, 4)` and the output of the `pdb` window is:\n","\n","```\n","-> out = self.classifier(h)\n","(Pdb) p h.shape\n","torch.Size([34, 4])\n","(Pdb) p self.classifier\n","Linear(in_features=2, out_features=4, bias=True)\n","exit\n","```\n","\n","From this we see that `self.classifier` expects two features, i.e. the input dimension should be `(N, 2)`, but we are passing a dimension of `(N, 4)`. \n","\n","Once we fix this issue, we will finally arrive at a bug free model. Execute the next cell to generate the bug free definition of the model and, finally, execute the training cell one last time!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EU0Uh2xmMrC5"},"outputs":[],"source":["import torch\n","import pdb\n","from torch.nn import Linear, Sequential, Tanh, ModuleList\n","from torch_geometric.nn import GCNConv\n","\n","\n","class BuggyGCN(torch.nn.Module):\n","    def __init__(self, args, dataset):\n","        super(BuggyGCN, self).__init__()\n","        torch.manual_seed(12345)\n","        \n","        self.convs = ModuleList([\n","            GCNConv(dataset.num_features, args[\"hidden_size\"]),\n","            GCNConv(args[\"hidden_size\"], args[\"hidden_size\"]),\n","            GCNConv(args[\"hidden_size\"], args[\"hidden_size\"])\n","        ])\n","\n","        self.classifier = Linear(args[\"hidden_size\"], dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","\n","        h = x\n","        for i in range(len(self.convs)):\n","          h = self.convs[i](h, edge_index)\n","          h = h.tanh()\n","\n","        # h = GNN embedding space.\n","        \n","        # Apply a final (linear) classifier.\n","        out = self.classifier(h)\n","\n","        return out, h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z_kwYrkOmmng"},"outputs":[],"source":["model = BuggyGCN(args, dataset)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"])  # Define optimizer.\n","\n","with tqdm(range(args[\"epochs\"]), unit=\"batch\") as tepoch:\n","  for epoch in tepoch:  \n","      loss, h = train(data)\n","      tepoch.set_description(f'Epoch {epoch} - Loss {loss.item()}')\n","      time.sleep(args['sleep'])"]},{"cell_type":"markdown","metadata":{"id":"xNZZwWVYjcid"},"source":["There shouldn't be any errors appearing and the loss should decrease with every epoch. \n","\n","Congratulations on finishing this tutorial and good luck with the class!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP3JAUIfmmnh"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"XCS224W_Colab0.ipynb","provenance":[{"file_id":"1fr5ut5-_vCz8CCTTe7lYqJUQjKZgemlt","timestamp":1654008744227}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":0}